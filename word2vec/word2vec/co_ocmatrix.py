# -*- coding: utf-8 -*-
"""co-ocMatrix.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CU0REp-z26kw6LRRk3w--b9pLEP-LHVe
"""

from nltk.corpus import reuters
from nltk.tokenize import sent_tokenize
from sklearn.decomposition import TruncatedSVD
from collections import defaultdict
from nltk.corpus import stopwords
from argparse import ArgumentParser
from nltk.stem import PorterStemmer 
from nltk.tokenize import word_tokenize 
from nltk.stem import WordNetLemmatizer 
  
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import re
import sys
import time
import string 
import os 

ps = PorterStemmer()
lm = WordNetLemmatizer()

from google.colab import drive
drive.mount('/content/gdrive')

import nltk
nltk.download('reuters')
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')

# currtime  = time.time()
# dirname = '/content/gdrive/My Drive/Colab Notebooks/word2vec'
# os.mkdir(dirname)

# dirname = '/content/gdrive/My Drive/Colab Notebooks/word2vec/output_data'
# os.mkdir(dirname)

dirname = '/content/gdrive/My Drive/Colab Notebooks/word2vec/output_data/3/'
# os.mkdir(dirname)

# str = ["123.45","avdbf-fs","asdfs's",'u.s.','23424.54','3232','34,35,343']
# for s in str:
#   print(re.match("^\d+?\.?,?\d+?,?-?\d+?$", s))
# reg = []
# [reg.append(s) for s in str if re.match("^\d+?\.?\d+?$", s) is None]
# print(reg)
# words = ['stem','stems','earn','earning','earner']
# words = ("U.S. And Japan has raised fears among many of Asia's exporting".lower()).split()
# stem_words = []
# [stem_words.append(lm.lemmatize(word)) for word in words if lm.lemmatize(word) not in stem_words]
# print(stem_words)
# # print()

class CoOccrenceMatrix:
    def processWords(self,words):
        stop_words = stopwords.words('english')
        words = [word for word in words if word not in stop_words] 
        removed = [word for word in words if re.match("^\d+?\.?,?-?\d+?$",word) is not None]
#         print(removed)
        modi_removed = [word for word in words if re.match("^\d+?\.?,?\d+?,?-?\d+?$",word) is not None]
#         print('in removed but not in modi_removed')
        w1 = [word for word in removed if word not in modi_removed]
#         print(w1)
#         print('-----')
#         print('in modi_removed but not in removed')
        w2 = [word for word in modi_removed if word not in removed]
#         print(w2)
        words = [word for word in words if re.match("^\d+?\.?,?-?\d+?$", word) is None ]
        
        stem_words = []
        [stem_words.append(lm.lemmatize(word)) for word in words if lm.lemmatize(word) not in stem_words]
        
        uniquewords = []
        [uniquewords.append(word) for word in stem_words if word not in uniquewords]
        return uniquewords
        
    def getWordList(self):
        print('getWordList')
        rawcontent = reuters.raw()[:1000]
        rawcontent = rawcontent.lower()
        words = rawcontent.split()
        print(words)
        filtered_words = self.processWords(words)
        sentences = rawcontent.split('.\n')

        np.save(os.path.join(dirname,'filtered_words'),filtered_words)
        np.save(os.path.join(dirname,'sentences'),sentences)
        print('filtered_word size : ',len(filtered_words))
        return list(filtered_words),sentences

    def getData(self): 
        
        start = time.time()
        words,sentences = self.getWordList()
        print('----',words)
        end = time.time()
        length = len(words)
        
        wordmap = defaultdict(list)
        i = 0
        for word in words:
            wordmap[word] = i 
            i += 1
        print(wordmap)
        np.save(os.path.join(dirname,'wordlist'),words)
        return wordmap,sentences

    def createCoOccurenceMatrix(self,k,sentences,wordmap):
        print('populating co occurence matrix if ',len(wordmap),'*',len(wordmap))
        length = int(len(wordmap))
        # print(wordmap)
        stop_words = stopwords.words('english')
        matrix = np.zeros((length, length),np.float64)
        print(k)
        for sentence in sentences:
            words = sentence.split()
            words = self.processWords(words)
#             print(len(wordmap))
            for i in range(len(words)):
                iword = words[i]
                for j in range(i-k,i+k+1):
                    if j >= 0 and j < len(words) and i != j:
                        jword = words[j]
                        if iword in wordmap and jword in wordmap:
                          matrix[wordmap[iword],wordmap[jword]] += 1
        print(matrix)
        return matrix

    def save_matrix(self,k,d,filename,save):
        print('save_matrix')
        wordmap,sentences = self.getData()
        print('vocabulary : ',len(wordmap))
        if(save):
            print('wordmap size:',len(wordmap))
            matrix = self.createCoOccurenceMatrix(k,sentences,wordmap)
            print('wordmap size:',len(wordmap))
            np.save(os.path.join(dirname,filename),matrix)
#             print(len(matrix[0]))
        print('------------------------------vocab size: ',len(wordmap))
        return matrix,wordmap

    def SVD(self,matrix,dimensions):
      print('svd on co occurence matrix')
      # TruncatedSVD(algorithm='randomized', n_components=5, n_iter=7,random_state=42, tol=0.0)
      svd = TruncatedSVD(n_components = dimensions)
      svdmatrix = svd.fit_transform(matrix)
      return svdmatrix

def main(k,d):
    print('in main')
    cooc = CoOccrenceMatrix()
    matrix,wordmap = cooc.save_matrix(k,d,"test_matrix",True)
    red_matrix = cooc.SVD(matrix,d)
#     print(red_matrix.shape)
    d1_dim,d2_dim = red_matrix[:0],red_matrix[:,1]
    for word in wordmap.keys():
      features = red_matrix[wordmap[word]]
      d1,d2 = features[0],features[1]
#       plt.scatter(d1,d2,marker='x',color='red')
#       plt.text(d1,d2,word)



# main(k,d)
main(3,2)

"""[[0. 1. 1. 1. 0. 0. 0. 0.]
 [1. 0. 1. 1. 1. 0. 0. 0.]
 [1. 1. 0. 1. 1. 1. 0. 0.]
 [1. 1. 1. 0. 1. 1. 1. 0.]
 [0. 1. 1. 1. 0. 1. 1. 0.]
 [0. 0. 1. 1. 1. 0. 1. 0.]
 [0. 0. 0. 1. 1. 1. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0.]]

[[0. 1. 1. 1. 0. 0. 0. 0.]
 [1. 0. 1. 1. 1. 0. 0. 0.]
 [1. 1. 0. 1. 1. 1. 0. 0.]
 [1. 1. 1. 0. 1. 1. 1. 0.]
 [0. 1. 1. 1. 0. 1. 1. 0.]
 [0. 0. 1. 1. 1. 0. 1. 0.]
 [0. 0. 0. 1. 1. 1. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0.]]
"""

print(ps.stem('fear'))
print('fear' in stopwords.words('english'))

cooc = CoOccrenceMatrix()