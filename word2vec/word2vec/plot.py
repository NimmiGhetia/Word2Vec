# -*- coding: utf-8 -*-
"""plot.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iuOZxqVJ9pNKlfudHGRRA7BZkOiuLeML
"""

# Commented out IPython magic to ensure Python compatibility.
import sys
assert sys.version_info[0]==3
assert sys.version_info[1] >= 5

from gensim.models import KeyedVectors
from gensim.test.utils import datapath
import pprint
import matplotlib.pyplot as plt
plt.rcParams['figure.figsize'] = [9,3]
import nltk
nltk.download('reuters')
from nltk.corpus import reuters
import numpy as np
import random
import scipy as sp
from sklearn.decomposition import TruncatedSVD
from sklearn.decomposition import PCA
# %matplotlib inline

START_TOKEN = '<START>'
END_TOKEN = '<END>'

np.random.seed(0)
random.seed(0)
# ----------------

def mapEmbedding():
  from collections import defaultdict
  wv_from_file = defaultdict(list)
  from google.colab import drive
  drive.mount('/content/gdrive',force_remount=True)
  f = open('/content/gdrive/My Drive/Colab Notebooks/word2vec/google/vectors_w5_f30','r')
  first = 0
  for line in f.readlines():
    if first == 0:
      first = 1 
      continue 
    words = line.split()
    key = words[0]
    values = []
    for value in words[1:]:
      values.append(float(value))
    wv_from_file[key] = list(values)
    
  return wv_from_file

wv_from_file = mapEmbedding()

def plot_embeddings(d=2):
  
    plt.figure(figsize=(12, 12))
    vocab = list(wv_from_file.keys())
    M = []
    for word in vocab:
      M.append(wv_from_file[word])
      
    n_iters = 10     
    M_reduced = None
    svd = TruncatedSVD(n_components=d, n_iter=n_iters)
    M_reduced = svd.fit_transform(M)
    
    x_coords = M_reduced[:, 0]
    y_coords = M_reduced[:, 1]
    testvocab1 = ['central','bank','company','prices','tax','interest','dollar','dlrs']
#     testvocab2 = ['sale','share','japan','wheat','shares','money','net','company','trade','profit','sales','price','total','rate','exports','dollar','dlr','prices','interest','production','imports','japanese','tax','earnings','rates','central','bank','market','stock','offer','exchange']
    testvocab3 = ['share','shares','sale','sales','share','shares','price','prices','export','exports']
    testvocab4 = ['money','net','company','price','total','sales','tax','rates','production']
    testvocab5 = ['crude','oil','natural','gas']
  
    for word in testvocab1:
        embedding = wv_from_file[word]
        x = embedding[0]
        y = embedding[1]
        plt.subplot(221)
        plt.scatter(x, y, marker='x', color='red')
        plt.text(x, y, word, fontsize=9)

    for word in testvocab3:
        embedding = wv_from_file[word]
        x = embedding[0]
        y = embedding[1]
        plt.subplot(222)
        plt.scatter(x, y, marker='o', color='blue')
        plt.text(x, y, word, fontsize=9)

    for word in testvocab4:
        embedding = wv_from_file[word]
        x = embedding[0]
        y = embedding[1]
        plt.subplot(223)
        plt.scatter(x, y, marker='*', color='green')
        plt.text(x, y, word, fontsize=9)

    for word in testvocab5:
        embedding = wv_from_file[word]
        x = embedding[0]
        y = embedding[1]
        plt.subplot(224)
        plt.scatter(x, y, marker='v', color='purple')
        plt.text(x, y, word, fontsize=9)
        
    # ------------------
plot_embeddings(2)

def cosine_similarity(a,b):
  from numpy import dot
  from numpy.linalg import norm
  cos_sim = dot(a, b)/(norm(a)*norm(b))
  return cos_sim

def findNearest(word1,word2,word3):
    from collections import defaultdict
    predictedword = np.array(wv_from_file[word1],dtype=np.float64) - np.array(wv_from_file[word2],dtype=np.float64) + np.array(wv_from_file[word3],dtype=np.float64)
    mapping = defaultdict(list)
    
    for word in wv_from_file.keys():
      if word != word1 and word != word2 and word != word3:
        cos_sim = cosine_similarity(np.array(wv_from_file[word],dtype=np.float64),predictedword)
        mapping[word] = cos_sim
      
    sorted_words = sorted(mapping.items(),key = lambda k: k[1],reverse = True)
    topk = 5
    i = topk
    print('----')
    for k,v in sorted_words:
      i -= 1
      print(k,' : ',v)
      if i == 0:
        break 

findNearest('gas','natural','oil')
findNearest('oil','crude','gas')

def mostSimilar(targetword,topk):
  from collections import defaultdict
  mapping = defaultdict(list)
  for word in wv_from_file.keys():
      cos_sim = cosine_similarity(np.array(wv_from_file[word],dtype=np.float64),np.array(wv_from_file[targetword],dtype=np.float64))
      mapping[word] = float(cos_sim)
  
  sorted_words = sorted(mapping.items(),key = lambda k: k[1],reverse = True)
  
  i = topk
  print('----')
  for k,v in sorted_words:
    i -= 1
    print(k,' : ',v)
    if i == 0:
      break 
      
print(wv_from_file['tax'])
mostSimilar('tax',5)
print(wv_from_file['bank'])
mostSimilar('bank',5)
mostSimilar('city',5)
mostSimilar('income',5)
mostSimilar('sells',5)
mostSimilar('oil',5)
mostSimilar('crude',5)
mostSimilar('natural',5)
mostSimilar('gas',5)