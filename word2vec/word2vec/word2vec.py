# -*- coding: utf-8 -*-
"""w2v.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gxjWxqDEh04SbpYT39SpJzD9L7iKgYJx
"""

import numpy as np
import math
import os
import time
import threading
import array as arr
import string

STRINGSIZE_LIMIT = 100
TABLE_SIZE = 1000
EXP_MAX = 6
MAX_SENTENCE_LIMIT = 1000
MAX_CODE_LIMIT = 40

dirname = '/content/gdrive/My Drive/Colab Notebooks/word2vec/google/'

vocabulary_hash_size =  30000000
class vocabulary_word:
    def __init__(self):
        self.count = 0
        self.point = 0
        self.word = ""
        self.code = "" 
        self.codelen = 0
    def print(self):
      print(self.count,' ',self.word, ' ',self.codelen)

train_file = os.path.join(dirname,'reuters_modified.txt')
output_file = os.path.join(dirname,'output')
save_vocabulary_file = os.path.join(dirname,'vocab')
read_vocabulary_file = os.path.join(dirname,'vocab')
read_vocabulary_fg = 0
save_vocabulary_fg = 0
output_fg = 0

binary = 0 
cbow  = 0
window = 5
min_count = 2
num_threads = 12
global min_reduce
vocabulary_max_size  = 1000000
vocabulary_size  = 0
layer1_size = 100
train_words = 0
word_count_actual = 0
itr = 10
file_size = 0
classes = 0
alpha = 0.025
starting_alpha = alpha 
sample = 1e-3
hs = 0
negative = 25
table_size = 1e8
table = []
for i in range(int(table_size)):
  table.append(0)
vocabulary = [] 
w0 = []
w1 = []
w1neg = []

# for i in range(vocabulary_max_size):
#   vocabulary.append(vocabulary_word())
vocabulary_hash = np.zeros(vocabulary_hash_size, dtype = np.integer)
expTable = []
for i in range(TABLE_SIZE*2 - 1):
  expTable.append(0.0)

def init_unigram_table():
  print('init unigram table')
  train_pow = 0
  d1 = 0.75
  power = 0.75
  global table_size
  global vocabulary_size
  for a in range(vocabulary_size):
    train_pow += math.pow(vocabulary[a].count, power)
  i = 0
  d1 = pow(vocabulary[i].count, power) / train_pow
  for a in range(int(table_size)):
    table[a] = i;
    if (a / table_size > d1):
      i += 1
      d1 += pow(vocabulary[i].count, power) / train_pow
    if (i >= vocabulary_size):
      i = vocabulary_size - 1

def read_word(fin):
#   print('read word')
  a = 0
  word = []
  while True:
    c = fin.read(1)
    if not c:
      break
    if ord(c) == 13:
      continue 
    if c == ' ' or c == '\t' or c == '\n':
      if a > 0:
        return "".join(word) 
      if c == '\n':
        word = '</s>'
        return word
      else:
        continue
        
    word.append(c) ;
    a += 1 
    if a >= STRINGSIZE_LIMIT-1:
      a -= 1
  return "".join(word)

def get_hash(word):
  hash = 0
  for a in range(len(word)):
    hash = hash*257 + ord(word[a])
  hash = hash % vocabulary_hash_size
  return hash

def search_vocabulary(word):
  hash = get_hash(word)
  while True:
    if vocabulary_hash[hash] == -1:
      return -1
    if word == vocabulary[vocabulary_hash[hash]].word:
      return vocabulary_hash[hash]
    hash = (hash + 1) % vocabulary_hash_size
  return -1

def read_word_index(fin):
  word = read_word(fin)
  c = fin.read(1)
  if not c:
    return -1
  return search_vocabulary(word)

def add_word_to_vocabulary(word):
  length = len(word)
  length += 1
  if length > STRINGSIZE_LIMIT:
    length = STRINGSIZE_LIMIT

  global vocabulary_size
#   print(vocabulary_size)
  vocabulary[vocabulary_size].word = word
  vocabulary[vocabulary_size].count = 0
  vocabulary_size += 1
  global vocabulary_max_size
  if vocabulary_size + 2 >= vocabulary_max_size:
    vocabulary_max_size += 1000
  hash = get_hash(word)
  while vocabulary_hash[hash] != -1:
    hash = (hash+1) % vocabulary_hash_size
  vocabulary_hash[hash] = vocabulary_size - 1
#   print('vocabulary size = ',vocabulary_size)
  return vocabulary_size - 1

def comp(voc):
  return voc.count

def sort_vocabulary():
  print('sort vocabulary')
  global vocabulary
  global train_words
  vocabulary = sorted(vocabulary,key = comp,reverse = True)
  for a in range(vocabulary_hash_size):
    vocabulary_hash[a] = -1
  global vocabulary_size
  print('vocabulary size = ',vocabulary_size)

  size = vocabulary_size
  train_words = 0 
  global min_count
#   for a in range(size):
#     vc = vocabulary[a]
#     vc.print()
    
  for a in range(size):
    if vocabulary[a].count < min_count and a != 0:
      vocabulary_size -= 1
      vocabulary[a].word = None
    else:
      hash = get_hash(vocabulary[a].word)
      while vocabulary_hash[hash] != -1:
        hash = (hash + 1) % vocabulary_hash_size 
      vocabulary_hash[hash] = a
      train_words += vocabulary[a].count
  for a in range(vocabulary_size):
    vocabulary[a].code = np.chararray(MAX_CODE_LIMIT)
    vocabulary[a].point = np.zeros(MAX_CODE_LIMIT)
  print('after sorting')
  for a in range(vocabulary_size):
    vc = vocabulary[a]
    vc.print()
#   print('vocabulary size : ',vocabulary_size)
#   print('Train words : ',train_words)

def reduce_vocabulary():
  print('reduce vocabulary')
  global vocabulary_size
  for a in range(vocabulary_size):
    if vocabulary[a].count > min_reduce:
      vocabulary[b].count = vocabulary[a].count 
      vocabulary[b].word = vocabulary[a].word
      b += 1
    else:
      vocabulary[a].word = None
  vocabulary_size = b
  for a in range(vocabulary_hash_size):
    vocabulary_hash[a] = -1
  for a in range(vocabulary_size):
    hash = get_hash(vocabulary[a].word)
    while vocabulary_hash[hash] != -1:
      hash = (hash + 1) % vocabulary_hash_size
    vocabulary_hash[hash] = a
  min_reduce += 1

def generate_binary_tree():
  print('create binary tree')
  point = []
  code = []
  count = []
  binary = []
  parent_node = []
  global vocabulary_size
  for a in range(MAX_CODE_LIMIT):
    code.append(0)
    point.append(0)
  for a in range(vocabulary_size*2 + 1):
    binary.append(0)
    count.append(0)
    parent_node.append(0)
  
  for a in range(vocabulary_size):
    count[a] = vocabulary[a].count
    
  for a in range(vocabulary_size,vocabulary_size*2):
    count[a] = 1e15
    
  pos1 = vocabulary_size - 1
  pos2 = vocabulary_size
  
  for a in range(vocabulary_size-1):
    if pos1 >= 0:
      if count[pos1] < count[pos2]:
        min1i = pos1 
        pos1 -= 1
      else:
        min1i = pos2 
        pos2 += 1
    else:
      min1i = pos2 
      pos2 += 1
    if pos1 >= 0:
      if count[pos1] < count[pos2]:
        min2i = pos1
        pos1 -= 1
      else:
        min2i = pos2 
        pos2 += 1
    else:
      min2i = pos2
      pos2 += 1
    count[vocabulary_size + a] = count[min1i] + count[min2i]
    parent_node[min1i] = vocabulary_size + a
    parent_node[min2i] = vocabulary_size + a
    binary[min2i] = 1
  
  for a in range(int(vocabulary_size)):
    b = a
    j = 0
    while True:
      code[j] = binary[b]
      point[j] = b
      j += 1
      b = parent_node[b]
      if b == vocabulary_size*2 - 2:
        break
      
    vocabulary[a].codelen = j
    vocabulary[a].point[0] = vocabulary_size - 2
    for b in range(j):
      vocabulary[a].code[j-b-1] = code[b]
      vocabulary[a].point[j-b] = point[b] - vocabulary_size 
  
  count.clear()
  binary.clear()
  parent_node.clear()

def learn_vocabulary():
  print('learn vocabulary from train file')
  for a in range(vocabulary_hash_size):
    vocabulary_hash[a] = -1
    
  fin = open(train_file,'r')
  global vocabulary_size
  global train_words
  
  add_word_to_vocabulary("</s>")

  word = ""
    
  while True:
    if train_words > 150000:
      break
    word = read_word(fin)
    if not word:
      break
    train_words += 1 
    if train_words % 100000 == 0:
      print(train_words/1000, 13)
    
    i = search_vocabulary(word)
    if i == -1:
      a = add_word_to_vocabulary(word)
      vocabulary[a].count = 1
    else:
      vocabulary[i].count += 1
    if vocabulary_size > vocabulary_hash_size * 0.7:
      reduce_vocabulary()
    
  sort_vocabulary()
  print('vocabulary size = ',vocabulary_size)
  print('words in train file : ',train_words)
  global file_size
  file_size = fin.tell()
  fin.close()

def save_vocabulary():
  f = open(save_vocabulary_file,'w')
  global vocabulary_size
  for j in range(vocabulary_size):
    st = str(vocabulary[j].word) + "," + str(vocabulary[j].count)
    f.write(st)
  f.close()

def read_vocabulary():
  print('read vocabulary')
  i = 0
  global vocabulary_size
  word = np.chararray(STRINGSIZE_LIMIT)
  fin = open(read_vocabulary_file,'r')
  for a in range(vocabulary_hash_size):
    vocabulary_hash[a] = -1
  vocabulary_size = 0 
  word = ""
  while True:
    word = read_word(fin)
#     print('---word---',word,'----')
    if not word:
      break 
    a = add_word_to_vocabulary(word)
    vocabulary[a].count = fin.readline()
    i += 1
  sort_vocabulary()
  print('vocabulary size : ',vocabulary_size)
  print('training word size : ',train_words)
  fin = open(train_file,'r')
  fin.seek(0,SEEK_END)
  global file_size
  file_size = fin.tell()
  fin.close()

def initialize_network():
  print('init net')
  next_random = 1
  global layer1_size
  global w0
  global vocabulary_size

  global hs
  if hs:
    for i in range(vocabulary_size*layer1_size):
      w1.append(0.0)
  if negative > 0:
    for i in range(vocabulary_size*layer1_size):
      w1neg.append(0.0)
  for a in range(vocabulary_size):
    for b in range(layer1_size):
      next_random = next_random*25214903917 + 11
      w0.append((((next_random & 0xFFFF) / 65536) - 0.5) / layer1_size)
  generate_binary_tree()

def train_model_perthread(th_id):
  print('train model thread')
  global vocabulary_size
  global file_size
  global starting_alpha
  global word_count_actual
  global itr
  global sample
  global window
  global cbow
  sentence_length = 0
  sentence_position = 0
  word_count = 0
  last_word_count = 0
  sen = []
  for i in range(MAX_SENTENCE_LIMIT+1):
    sen.append(0.0)
#     np.array(MAX_SENTENCE_LIMIT+1,dtype = np.float64)
  local_itr = itr
  next_random = th_id 
  now = time.time()
  hin = []
#   np.array(layer1_size,dtype = np.float64)
  hout = []
  for i in range(layer1_size):
    hin.append(0.0)
    hout.append(0.0)
#   np.array(layer1_size,dtype = np.float64)
  fi = open(train_file,'r')
  fi.seek(int(float(file_size)/num_threads * float(th_id)), os.SEEK_SET)
  while True:
    
    if word_count - last_word_count > 10000:
      word_count_actual += word_count - last_word_count
      last_word_count = word_count
      now = time.time()
    alpha = starting_alpha * (1 - word_count_actual/ (itr * train_words + 1))
    if alpha < starting_alpha * 0.001:
      alpha = starting_alpha * 0.001
    eof = False
    if sentence_length == 0:
      while True:
        word = read_word_index(fi)
        if not word:
          eof = True
          break 
        if word == -1:
          continue
        word_count += 1
        if word == 0:
          break 
        if sample > 0:
          ran = (math.sqrt(vocabulary[word].count / (sample * train_words)) + 1) * (sample * train_words) / vocabulary[word].count 
          next_random = next_random * 25214903917 + 11
          if ran < (next_random & 0xFFFF) / 65536 :
            continue 
        sen[sentence_length] = word
        sentence_length += 1
        if sentence_length > MAX_SENTENCE_LIMIT:
          break 
      sentence_position = 0
      
    
    if eof or word_count > train_words / num_threads:
      word_count_actual += word_count - last_word_count
      local_itr -= 1
      if local_itr == 0:
        break
      word_count = 0
      last_word_count = 0
      sentence_length = 0
      fi.seek(int(float(file_size) / num_threads * float(th_id)) , os.SEEK_SET)
      continue 
    word = sen[sentence_position]
    if word == -1:
      continue
    for i in range(len(hin)):
      nue1[i] = 0
    for i in range(len(hout)):
      hout[i] = 0
    next_random = next_random * 25214903917 + 11
    b = next_random % window
    if cbow:
      cw = 0
      for a in range(b,window*2+1-b):
        if a != window:
          c = sentence_position - window + a 
          if c < 0:
            continue 
          if c >= sentence_length:
            continue
          last_word = sen[c]
          for c in range(layer1_size):
            hin[c] += w0[c+last_word*layer1_size]
          cw += 1
      if cw:
        for c in range(layer1_size):
          hin[c] /= cw 
        if hs:
#           print('heirarchichal softmax')
          for d in range(vocabulary[word].codelen):
            f = 0
            l2 = vocabulary[word].point[d] * layer1_size
            for c in range(layer1_size):
              f += hin[c] * w1[c+l2]
            if f <= -EXP_MAX or f >= EXP_MAX:
              continue
            else:
              f = expTable[int((f+EXP_MAX) * (TABLE_SIZE / EXP_MAX / 2))]
            g = (1 - vocabulary[word].code[d] - f) * alpha
            for c in range(layer1_size):
              hout[c] += g * w1[c+l2]
            for c in range(layer1_size):
              w1[c+l2] += g * hin[c]
        if negative > 0:
#           print('negative sampling')
          for d in range(negative+1):
            if d == 0:
              target = word
              label = 1
            else:
              next_random = next_random * 25214903917 + 11
              target = table[int((next_random >> 16) % table_size)]
              if target == 0:
                target = next_random * (vocabulary_size - 1) + 1
              if target == word:
                continue
              label = 0
            l2 = target * layer1_size
            f = 0
            for c in range(layer1_size):
              f += hin[c] * w1neg[c+l2]
            if f > EXP_MAX:
              g = (label - 1) * alpha
            elif f < -EXP_MAX:
              g = (label - 0) * alpha
            else:
              g = (label - expTable[(f + EXP_MAX) * (TABLE_SIZE / EXP_MAX / 2)]) * alpha
            for c in range(layer1_size):
              hin2[c] += g * w1neg[c+l2]
            for c in range(layer1_size):
              w1neg[c + l2] += g * hin[c]
         
        for a in range(b,window*2+1-b):
          if a != window:
            c = sentence_position - window + a
            if c < 0:
              continue
            if c >= sentence_length:
              continue
            last_word = sen[c]
            if last_word != -1:
              continue
            l1 = last_word * layer1_size
            for c in range(layer1_size):
              w0[c+last_word*layer1_size] += hout[c] 
    else:
      print('skip-gram')
      for a in range(b,window*2+1-b):
        if a != window:
          c = sentence_position - window + a
          if c < 0 or c >= sentence_length:
            continue
          last_word = sen[c]
          if last_word == -1:
            continue
          l1 = last_word * layer1_size
          for c in range(layer1_size):
            hout[c] = 0
          if hs:
#             print('heirarchical softmax')
            for d in range(vocabulary[word].codelen):
              f = 0
              l2 = vocabulary[word].point[d] * layer1_size
              for c in range(layer1_size):
                f += w0[c+l1] * w1[c+l2]
              if f <= -EXP_MAX or f >= EXP_MAX:
                continue
              else:
                f = expTable[int((f+EXP_MAX) * (TABLE_SIZE / EXP_MAX / 2))]
              g = (1 - vocabulary[word].code[d] - f) * alpha
              for c in range(layer1_size):
                hout[c] += g * w1[c+l2]
              for c in range(layer1_size):
                w1[c+l2] += g * w0[c+l1]
          if negative > 0:
            print('negative sampling')  
            for c in range(negative + 1):
              if c == 0:
                target = word
                label = 1
              else:
                next_random = next_random * 25214903917 + 11
                target = table[int((next_random >> 16) % table_size)]
                if target == 0:
                  target = next_random % (vocabulary_size - 1) + 1
                if target == word:
                  continue
                label = 0
              l2 = target * layer1_size
              f = 0
#               print('forward propogation')
              for c in range(layer1_size):
                f += w0[c + l1] * w1neg[c + l2]
#                 print(c+l1, ' , ', c+l2)
#               print('gradient')
              if f > EXP_MAX:
                g = (label - 1) * alpha
              elif f < -EXP_MAX:
                g = (label - 0) * alpha
              else:
                g = (label - expTable[int((f+EXP_MAX) * (TABLE_SIZE / EXP_MAX / 2))]) * alpha
#               print('backward propogation')
              for c in range(layer1_size):
                hout[c] += g * w1neg[c + l2]
              for c in range(layer1_size):
                w1neg[c + l2] += g * w0[c + l1]

          for c in range(layer1_size):
            w0[c+l1] += hout[c]

    sentence_position += 1
    if sentence_position >= sentence_length:
      sentence_length = 0
      continue
  print('training done')
  fi.close()
  hin.clear()
  hout.clear()

def train_model():
  print('train model')
  t = []
  global start_alpha
  global vocabulary_size
  global itr
  starting_alpha = alpha
  if read_vocabulary_fg != 0:
    Read_vocabulary()
  else:
    learn_vocabulary()
  if save_vocabulary_file[0] != 0:
    save_vocabulary()
  if output_file[0] == 0:
    return
  initialize_network()
  if negative > 0:
    init_unigram_table()
  start = time.time()
  global num_threads  
  
  for i in range(num_threads):
    nm = 't'+str(i)
    t.append(threading.Thread(target = train_model_perthread,name = nm,args=(i,)))
  for i in range(num_threads):
    t[i].start()
  for i in range(num_threads):
    t[i].join()
#   train_model_perthread(1) 
  fo = open(output_file,'w')
  global classes
  if classes == 0:
    fo.write('%ld %ld\n' % (vocabulary_size,layer1_size))
    for a in range(vocabulary_size):
      fo.write("%s\t" % vocabulary[a].word)
      global binary
      if binary:
        for b in range(layer1_size):
          fo.write(w0[a*layer1_size+b])
          fo.write(' ')
      else:
        for b in range(layer1_size):
          fo.write(str(w0[a*layer1_size+b]))
          fo.write(' ')
      fo.write('\n')
  else:
    clcn = classes
    itr = 10
    centcn = np.array(classes)
    cl = np.zeros(vocabulary_size)
    cent = np.zeros(vocabulary_size*layer1_size,dtype = np.float64)
    for a in range(vocabulary_size):
      cl[a] = a % clcn
    for a in range(itr):
      for b in range(clcn*layer1_size):
        centcn[b] = 0
      for b in range(clcn):
        centcn[b] = 1
      for c in range(vocabulary_size):
        for d in range(layer1_size):
          cent[layer1_size * cl[c] + d] += w0[c * layer1_size + d]
        centcn[cl[c]] += 1
    for b in range(clcn):
      closev = 0
      for c in range(layer1_size):
        cent[layer1_size*b + c] /= centcn[b]
        closev += cent[layer1_size * b + c] * cent[layer1_size * b + c]
      closev = math.sqrt(closev)
      for c in range(layer1_size):
        cent[layer1_size * b + c] /= closev
    for c in range(vocabulary_size):
      closev = -10
      closeid = 9
      for d in range(clcn):
        x = 0
        for b in range(layer1_size):
          x += cent[layer1_size * d + b] * w0[c * layer1_size + b]
        if c > closev:
          closev = x 
          closeid = d
      cl[c] = closeid
    for a in range(vocabulary_size):
      fo.write('%s %d\n',vocabulary[a].word,cl[a])
    centcn.clear()
    cent.clear()
    cl.clear()
  fo.close()

default_training_file = os.path.join(dirname,'reuters_modified.txt')
default_save_vocabulary_file = os.path.join(dirname,'vocabulary')
default_read_vocabulary_file = os.path.join(dirname,'vocabulary')
default_output_file = os.path.join(dirname,'output.txt')

def main(layer1_size=30,binary=0,cb=0,alpha=0.75,wnd=5,smple=1e-4,hs=1,negative=25,num_threads=12,itrt=10,min_count=10,classes=0):
  print('main')
  global itr
  global train_words
  global vocabulary_size
  global word_count_actual
  global sample
  global window
  global cbow 
  cbow = cb
  window = wnd
  sample = smple
  itr = itrt
  output_file_fg = 0 
  save_vocabulary_fg = 0 
  read_vocabulary_fg = 0
  train_words = 0
  word_count_actual = 0
  global file_size
  file_size = 0
  table_size = 1e8
  vocabulary_size = 0
  train_words = 0
  train_file = default_training_file
  save_vocabulary_file = default_save_vocabulary_file
  read_vocabulary_file = default_read_vocabulary_file
  output_file = default_output_file
  for a in range(vocabulary_max_size):
    vocabulary.append(vocabulary_word())
    
  if cbow:
    alpha = 0.05
  for i in range(TABLE_SIZE):
    expTable[i] = math.exp((i / TABLE_SIZE * 2 - 1)*EXP_MAX)
    expTable[i] = expTable[i] / (expTable[i] + 1)
  train_model()
  print('check output file for feature vectors')

from google.colab import drive
drive.mount('/content/gdrive',force_remount=True)
main()